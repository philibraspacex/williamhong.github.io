{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7374826,"sourceType":"datasetVersion","datasetId":4285180},{"sourceId":15929070,"sourceType":"kernelVersion"}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import inception_resnet_v2\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport numpy as np\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nfrom sklearn.metrics import confusion_matrix\nimport seaborn as sns","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install matplotlib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\n\nfingerprint_image = cv2.imread('/kaggle/input/soco-fingerprint-female-and-male/SOCOFing/Train/Male/100__M_Left_index_finger.BMP', cv2.IMREAD_GRAYSCALE)\n\nthreshold_value = 127\nbinary_image = cv2.threshold(fingerprint_image, threshold_value, 255, cv2.THRESH_BINARY)\ncv2.imshow('Original Fingerprint', fingerprint_image)\ncv2.imshow('Binary Fingerprint', binary_image)\ncv2.waitKey(0)\ncv2.destroyAllWindows()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\n\nimg = cv2.imread(\"/kaggle/input/fingerprint/1.jpg\", 0)\nprint(img)  # Check if img is loaded correctly\ncv2.imshow('enhanced_image', img)\ncv2.waitKey(0)\ncv2.destroyAllWindows()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport PIL\nimport fingerprint_enhancer\nimport numpy as np\n\n# Read the image using PIL\nimg = PIL.Image.open(\"/kaggle/input/fingerprint/4__M_Left_thumb_finger.BMP\")\n\n# Convert the image to grayscale\ngray_img = img.convert(\"L\")\n\n# Resize the image\nscaling_factor = 4  # Adjust the scaling factor as needed\nresized_img = gray_img.resize((gray_img.width * scaling_factor, gray_img.height * scaling_factor))\n\n# Convert resized PIL Image to NumPy array\nresized_img_array = np.array(resized_img)\n\n# Convert original grayscale PIL Image to NumPy array\noriginal_img_array = np.array(gray_img)\n\n# Enhance the fingerprint image\nout = fingerprint_enhancer.enhance_Fingerprint(resized_img_array)\n\n# Display the original grayscale image, resized image, and the enhanced image side by side\nplt.figure(figsize=(15, 5))\n\n# Original grayscale image\nplt.subplot(1, 3, 1)\nplt.imshow(original_img_array, cmap='gray')\nplt.title('Original Grayscale Image')\nplt.axis('off')\n\n# Resized image\nplt.subplot(1, 3, 2)\nplt.imshow(resized_img_array, cmap='gray')\nplt.title('Resized Image')\nplt.axis('off')\n\n# Enhanced image\nplt.subplot(1, 3, 3)\nplt.imshow(out, cmap='gray')\nplt.title('Enhanced Image')\nplt.axis('off')\n\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport numpy as np\n\ndef enhance_fingerprint(image_path, output_path):\n    # Baca gambar sidik jari\n    fingerprint_image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n    \n    # Menerapkan operasi morfologi untuk memperbaiki struktur sidik jari\n    kernel = np.ones((5,5), np.uint8)\n    opening = cv2.morphologyEx(fingerprint_image, cv2.MORPH_OPEN, kernel)\n    \n    # Menerapkan operasi top-hat untuk meningkatkan detail halus\n    top_hat = cv2.morphologyEx(fingerprint_image, cv2.MORPH_TOPHAT, kernel)\n    \n    # Gabungkan gambar asli dengan hasil top-hat\n    enhanced_image = cv2.add(fingerprint_image, top_hat)\n    \n    # Simpan gambar hasil\n    cv2.imwrite(output_path, enhanced_image)\n    \n    print(\"Fingerprint enhancement complete. Enhanced image saved as\", output_path)\n\n# Contoh penggunaan\ninput_image_path = \"/kaggle/input/soco-fingerprint-female-and-male/SOCOFing/ForTestingModel/Female/2__F_Left_index_finger.BMP\"\noutput_image_path = \"enhanced_fingerprint.jpg\"\nenhance_fingerprint(input_image_path, output_image_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"real_files = os.listdir(train_male_dir)\nprint(real_files[:10])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline\n\npic_index = 3\n\n\nnext_train = [os.path.join(train_male_dir, fname) \n                for fname in real_files[pic_index-2:pic_index]]\n\n\nfor i, img_path in enumerate(next_train):\n  img = mpimg.imread(img_path)\n  plt.imshow(img)\n  plt.axis('Off')\n  plt.show()\n    \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_val_generators(training_dir, validation_dir, testing_dir):\n\n    train_datagen = ImageDataGenerator(rescale=1./255.,\n                                       rotation_range=40,\n                                       width_shift_range=0.2,\n                                       height_shift_range=0.2,\n                                       shear_range=0.2,\n                                       zoom_range=0.2,\n                                       horizontal_flip=True,\n                                       fill_mode='nearest')\n\n    train_generator = train_datagen.flow_from_directory(directory=training_dir,\n                                                        batch_size=64,\n                                                        class_mode='categorical',\n                                                        target_size=(224, 224))\n\n    validation_datagen = ImageDataGenerator(rescale=1./255.)\n\n    validation_generator = validation_datagen.flow_from_directory(directory=validation_dir,\n                                                                  batch_size=64,\n                                                                  class_mode='categorical',\n                                                                  target_size=(224, 224))\n    testing_datagen = ImageDataGenerator(rescale=1./255.)\n\n    testing_generator = testing_datagen.flow_from_directory(directory=testing_dir,\n                                                                  batch_size=64,\n                                                                  class_mode='categorical',\n                                                                  target_size=(224, 224))\n\n    return train_generator, validation_generator, testing_generator","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"training_dir = '/kaggle/input/soco-fingerprint-female-and-male/SOCOFing/Train'\nvalidation_dir = '/kaggle/input/soco-fingerprint-female-and-male/SOCOFing/Validasi'\ntesting_dir = '/kaggle/input/soco-fingerprint-female-and-male/SOCOFing/Testing'\ntrain_generator, validation_generator, testing_generator = train_val_generators(training_dir, validation_dir,testing_dir)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.applications import InceptionResNetV2\nInceptionResNetV2_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n\nfor layer in InceptionResNetV2_model.layers:\n    layer.trainable = False\n\nmodel = Sequential([\n    InceptionResNetV2_model,\n    GlobalAveragePooling2D(),\n    Dense(2, activation='softmax')\n])\nprint(model.summary())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.compile(\n    optimizer=Adam(learning_rate=0.001), \n    loss='binary_crossentropy', \n    metrics=['accuracy'])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(\n    train_generator,\n    steps_per_epoch=len(train_generator),\n    epochs=10,\n    validation_data=validation_generator,\n    validation_steps=len(validation_generator)\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"true_classes = testing_generator.classes\n\nclass_labels = list(testing_generator.class_indices.keys())\n\npredicted_classes = model.predict(testing_generator)\npredicted_classes = np.argmax(predicted_classes, axis=1)\n\nprecision, recall, f1_score, support = precision_recall_fscore_support(\n    true_classes, predicted_classes, average='weighted', labels=np.unique(predicted_classes))\n\naccuracy = accuracy_score(true_classes, predicted_classes)\n\nprint('Precision:', precision)\nprint('Recall:', recall)\nprint('F1-Score:', f1_score)\nprint('Accuracy:', accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cm = confusion_matrix(true_classes, predicted_classes)\n\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.title('Confusion Matrix')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend(['Training', 'Validation'], loc='upper left')\nplt.show()\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend(['Training', 'Validation'], loc='upper left')\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport os\nimport shutil\nfrom tensorflow.keras.utils import load_img, img_to_array\nfrom IPython.display import Image\nfrom PIL import Image as PILImage\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\n# Menggunakan PIL untuk membuka dan mengonversi gambar ke format RGB\nimg = Image.open(\"/kaggle/input/soco-fingerprint-female-and-male/SOCOFing/ForTestingModel/Female/2__F_Left_index_finger.BMP\").convert(\"RGB\")\n\n# Mengubah gambar PIL ke array NumPy\nimg_array = np.array(img)\n\n# Memeriksa bentuk array\nprint(img_array.shape)\n\n# Mengunggah file gambar\nuploaded_file = input(\"Masukkan path file gambar: \")\nuploaded_file_name = os.path.basename(uploaded_file)\n\n# Salin file gambar ke direktori saat ini\nshutil.copyfile(uploaded_file, uploaded_file_name)\n\n# Membaca gambar menggunakan PIL\nimg = PILImage.open(uploaded_file_name)\nimg = img.resize((224, 224))  # Mengubah ukuran gambar menjadi (150, 150)\n\nplt.imshow(img)\nplt.axis('off')\nplt.show()\n\n# Mengonversi gambar menjadi array dan melakukan normalisasi\nx = img_to_array(img)\nx /= 255\nx = np.expand_dims(x, axis=0)\n\n# Melakukan prediksi gambar\npredictions = model.predict(x)\n\n# Menentukan label kelas\nclass_labels = ['Female', 'Male']\npredicted_class = class_labels[np.argmax(predictions)]\nconfidence = np.max(predictions) * 100\n\n# Menampilkan hasil prediksi\nprint(f\"{uploaded_file_name} terdeteksi sebagai: {predicted_class}\")\nprint(f\"Confidence: {confidence:.2f}%\")\n\n# Menghapus file yang diunggah\nos.remove(uploaded_file_name)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}